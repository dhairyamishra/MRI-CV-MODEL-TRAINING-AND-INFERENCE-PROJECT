# ============================================================================
# Phase 2.3: Joint Fine-Tuning Configuration - PRODUCTION
# ============================================================================
# Joint fine-tuning of all components with differential learning rates
# This is Stage 3 of 3-stage multi-task training strategy
# PRODUCTION MODE: Full dataset, 1000 epochs, ~4-6 hours
# 
# IMPORTANT: PyTorch 2.6+ requires weights_only=False when loading checkpoints
# IMPORTANT: Windows requires num_workers=0 for DataLoader

# Experiment metadata
experiment:
  name: "multitask_joint_production"
  description: "Phase 2.3 PRODUCTION - Joint fine-tuning with differential LR on full dataset"
  tags: ["multitask", "joint", "fine_tuning", "brats", "kaggle", "production"]
  notes: "Fine-tune all components jointly with differential learning rates for optimal performance"

# Data directories (changed from 'paths' to 'data' to match training function)
data:
  kaggle_train_dir: "data/processed/kaggle/train"
  kaggle_val_dir: "data/processed/kaggle/val"
  kaggle_test_dir: "data/processed/kaggle/test"
  brats_train_dir: "data/processed/brats2d_full/train"
  brats_val_dir: "data/processed/brats2d_full/val"
  brats_test_dir: "data/processed/brats2d_full/test"

# Checkpoint and output paths
paths:
  checkpoint_dir: "checkpoints/multitask_joint_production"
  log_dir: "logs/multitask_joint_production"
  output_dir: "outputs/multitask_joint_production"

# Model architecture
model:
  name: "multitask"
  in_channels: 1
  seg_out_channels: 1
  num_classes: 2              # Changed from cls_num_classes to match training function
  base_filters: 64
  depth: 4
  cls_hidden_dim: 256
  cls_dropout: 0.5

# Model initialization
init:
  path: "checkpoints/multitask_cls_head_production/best_model.pth"
  freeze_encoder: false   # Unfreeze all components
  freeze_seg_decoder: false
  freeze_cls_head: false

# Multi-task loss configuration
loss:
  # Segmentation loss
  seg_loss:
    name: "dice_bce"
    dice_weight: 0.6
    bce_weight: 0.4
    smooth: 1.0
  
  # Classification loss
  cls_loss:
    name: "cross_entropy"
    # class_weights: null   # Commented out - will use unweighted loss
    label_smoothing: 0.1
  
  # Loss weighting
  lambda_seg: 1.0         # Weight for segmentation loss
  lambda_cls: 0.5         # Weight for classification loss (lower to balance)

# Differential learning rates
optimizer:
  name: "adamw"
  encoder_lr: 0.0001      # Lower LR for encoder (already trained)
  decoder_cls_lr: 0.0003  # Medium LR for decoder and classifier (consolidated from seg_decoder_lr and cls_head_lr)
  weight_decay: 0.0001
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler
scheduler:
  name: "cosine"
  T_max: 1000             # Match number of epochs (changed from 50)
  eta_min: 1.0e-7

# Training hyperparameters
training:
  epochs: 1000            # Extended training for production (changed from 50)
  batch_size: 32          # Increased batch size (changed from 16)
  num_workers: 0          # Set to 0 for Windows compatibility (use 4+ on Linux/Mac)
  pin_memory: true
  
  # Differential learning rates (key for joint fine-tuning)
  encoder_lr: 0.0001      # 1e-4 (lower for fine-tuning pre-trained encoder)
  decoder_cls_lr: 0.0003  # 3e-4 (higher for task-specific heads)
  weight_decay: 0.0001    # L2 regularization
  
  # Mixed precision training
  use_amp: true
  mixed_precision: true   # Added for compatibility with training function
  
  # Gradient clipping
  grad_clip: 1.0
  
  # Early stopping (monitor combined metric)
  early_stopping:
    enabled: true
    patience: 50          # More patience for longer training (changed from 20)
    min_delta: 0.0005
    monitor: "val_combined" # Combined metric: (dice + acc) / 2
    mode: "max"
  
  # Checkpoint saving
  save_best: true
  save_last: true
  save_frequency: 5
  keep_last_n: 5
  save_optimizer: true
  save_scheduler: true

# Data augmentation (moderate for joint training)
augmentation:
  train:
    enabled: true
    random_flip_h: 0.5
    random_flip_v: 0.5
    random_rotate: 15
    random_scale: 0.1
    elastic_deform: true
    elastic_alpha: 40
    elastic_sigma: 4
    gaussian_noise: 0.015
    gaussian_blur: 0.25
    brightness: 0.15
    contrast: 0.15
    gamma: 0.15
  val:
    enabled: false

# Dataset mixing strategy
dataset:
  # Mix BraTS (with masks) and Kaggle (without masks)
  mix_datasets: true
  brats_ratio: 0.3        # 30% BraTS samples per batch
  kaggle_ratio: 0.7       # 70% Kaggle samples per batch
  
  # Sampling strategy
  sampling: "balanced"    # 'balanced' or 'weighted'

# Weights & Biases logging
wandb:
  enabled: true
  project: "slicewise-multitask-production"
  entity: null
  name: "joint_production"
  tags: ["production", "stage3", "joint", "fine_tuning"]
  notes: "Production joint fine-tuning with full dataset"
  
  # Log additional metrics
  log_gradients: true     # Log gradient norms
  log_weights: false      # Don't log weights (too large)
  log_frequency: 100      # Log every 100 batches

# Reproducibility
seed: 42

# Device
device: "cuda"

# Performance optimization
performance:
  # Gradient accumulation (if GPU memory limited)
  gradient_accumulation_steps: 1
  
  # Compile model (PyTorch 2.0+)
  compile_model: false    # Set to true if using PyTorch 2.0+
  
  # Benchmark mode
  cudnn_benchmark: true
  cudnn_deterministic: false
