# ============================================================================
# SliceWise - Production 2D Segmentation Training Configuration
# ============================================================================
# Optimized configuration for training highly accurate U-Net segmentation model
# Target: 100+ epochs with advanced training techniques

# Experiment metadata
experiment:
  name: "unet_brats_production_v1"
  description: "Production U-Net for brain tumor segmentation - 100 epochs with advanced optimization"
  tags: ["unet", "segmentation", "brats", "flair", "production", "100epochs"]
  notes: "Full training run on complete BraTS dataset with all optimizations"

# Paths
paths:
  train_dir: "data/processed/brats2d_full/train"
  val_dir: "data/processed/brats2d_full/val"
  test_dir: "data/processed/brats2d_full/test"
  checkpoint_dir: "checkpoints/seg_production"
  log_dir: "logs/seg_production"
  output_dir: "outputs/seg_production"
  visualization_dir: "visualizations/seg_production"

# Model architecture
model:
  name: "unet2d"
  in_channels: 1          # Single-channel MRI (FLAIR)
  out_channels: 1         # Binary segmentation
  base_filters: 64        # Base number of filters (try 32, 64, 128)
  depth: 4                # Number of encoder/decoder blocks
  use_bilinear: true      # Use bilinear upsampling (vs transposed conv)
  use_attention: false    # Add attention gates (experimental)
  use_deep_supervision: false  # Deep supervision for better gradients

# Loss function
loss:
  name: "dice_bce"        # Options: dice, bce, dice_bce, tversky, focal
  dice_weight: 0.6        # Weight for Dice loss (if using dice_bce)
  bce_weight: 0.4         # Weight for BCE loss (if using dice_bce)
  smooth: 1.0             # Smoothing factor for Dice loss
  # Tversky loss parameters (if using tversky)
  alpha: 0.3              # False positive weight
  beta: 0.7               # False negative weight (higher = penalize FN more)
  # Focal loss parameters (if using focal)
  focal_alpha: 0.25
  focal_gamma: 2.0

# Optimizer
optimizer:
  name: "adamw"           # Options: adam, adamw, sgd
  lr: 0.0005              # Learning rate (higher with warmup)
  weight_decay: 0.0001    # L2 regularization
  # Adam/AdamW specific
  betas: [0.9, 0.999]
  eps: 1.0e-8
  # SGD specific
  momentum: 0.9
  nesterov: true

# Learning rate warmup
warmup:
  enabled: true
  warmup_epochs: 5        # Gradually increase LR for first N epochs
  warmup_start_lr: 0.00001

# Learning rate scheduler
scheduler:
  name: "cosine"          # Options: cosine, step, plateau, onecycle, none
  # Cosine annealing
  T_max: 100              # Maximum number of iterations
  eta_min: 1.0e-7         # Minimum learning rate
  # Step scheduler
  step_size: 30           # Period of learning rate decay
  gamma: 0.1              # Multiplicative factor
  # ReduceLROnPlateau
  mode: "max"             # min or max (max for dice)
  factor: 0.5             # Factor by which LR is reduced
  patience: 10            # Number of epochs with no improvement
  threshold: 0.001        # Threshold for measuring improvement
  min_lr: 1.0e-7

# Training hyperparameters
training:
  epochs: 100
  batch_size: 16          # Adjust based on GPU memory
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  
  # Gradient accumulation (simulate larger batch size)
  gradient_accumulation_steps: 2  # Effective batch = 16 * 2 = 32
  
  # Mixed precision training
  use_amp: true           # Automatic Mixed Precision
  
  # Gradient clipping
  grad_clip: 1.0          # Max gradient norm (0 to disable)
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 25          # Epochs to wait for improvement
    min_delta: 0.001      # Minimum change to qualify as improvement
    monitor: "val_dice"   # Metric to monitor
    mode: "max"           # max or min
  
  # Checkpoint saving
  save_best: true         # Save best model based on val metric
  save_last: true         # Save last checkpoint
  save_frequency: 5       # Save every N epochs (0 to disable)
  keep_last_n: 5          # Keep more checkpoints for production
  save_optimizer: true
  save_scheduler: true

# Validation
validation:
  frequency: 1            # Validate every N epochs
  metric: "dice"          # Primary metric (dice, iou)
  compute_test_metrics: true  # Compute test metrics at end
  save_predictions: true  # Save prediction visualizations

# Data augmentation
augmentation:
  train:
    enabled: true
    random_flip_h: 0.5    # Horizontal flip probability
    random_flip_v: 0.5    # Vertical flip probability
    random_rotate: 20     # Random rotation degrees
    random_scale: 0.15    # Random scale factor
    elastic_deform: true  # Elastic deformation
    elastic_alpha: 50     # Elastic deformation strength
    elastic_sigma: 5      # Elastic deformation smoothness
    gaussian_noise: 0.02  # Gaussian noise std (0 to disable)
    gaussian_blur: 0.3    # Gaussian blur probability
    brightness: 0.2       # Brightness adjustment range
    contrast: 0.2         # Contrast adjustment range
    gamma: 0.2            # Gamma correction range
  val:
    enabled: false        # No augmentation for validation
  test:
    enabled: false        # No augmentation for test

# Logging
logging:
  use_wandb: true
  wandb_project: "slicewise-segmentation-production"
  wandb_entity: null      # Your W&B username (null for default)
  wandb_run_name: null    # Auto-generated if null
  log_frequency: 10       # Log every N batches
  log_images: true        # Log example predictions
  log_images_frequency: 5 # Log images every N epochs
  num_images_to_log: 8    # Number of images to log
  log_gradients: true     # Log gradient norms
  log_learning_rate: true
  
  # TensorBoard (alternative/additional to W&B)
  use_tensorboard: true
  tensorboard_dir: "runs/seg_production"

# Evaluation metrics
metrics:
  - "dice"                # Dice coefficient
  - "iou"                 # Intersection over Union
  - "precision"           # Precision
  - "recall"              # Recall (Sensitivity)
  - "f1"                  # F1 score
  - "specificity"         # Specificity
  - "hausdorff"           # Hausdorff distance (optional, expensive)

# Post-processing (for inference/evaluation)
postprocessing:
  enabled: true
  threshold: 0.5          # Probability threshold
  min_object_size: 50     # Remove objects smaller than N pixels
  fill_holes_size: 100    # Fill holes smaller than N pixels
  morphology_kernel: 3    # Morphological operations kernel size

# Reproducibility
seed: 42

# Device
device: "cuda"            # cuda or cpu (auto-detected if cuda available)

# Resume training
resume:
  enabled: false
  checkpoint_path: null   # Path to checkpoint to resume from
  resume_optimizer: true
  resume_scheduler: true
  resume_epoch: true

# Advanced options
advanced:
  # Online hard example mining
  ohem:
    enabled: false
    keep_ratio: 0.7       # Keep top 70% hardest examples
  
  # Exponential Moving Average of model weights
  ema:
    enabled: false
    decay: 0.999
  
  # Test-time augmentation
  tta:
    enabled: false        # Enable for final evaluation
    num_augmentations: 8
