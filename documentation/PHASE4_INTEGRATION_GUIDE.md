# Phase 4: Multi-Task Model Integration Guide

**Status:** âœ… COMPLETE  
**Date:** December 6, 2025  
**Version:** 1.0.0

---

## ðŸ“‹ Overview

Phase 4 integrates the trained multi-task model into the production application, enabling unified classification and segmentation in a single forward pass.

**Key Benefits:**
- ðŸš€ ~40% faster inference (single forward pass)
- ðŸ’¾ 9.4% fewer parameters (2.0M vs 2.2M)
- ðŸŽ¯ Conditional segmentation (smart resource usage)
- ðŸ“Š Excellent performance: 91.3% accuracy, 97.1% sensitivity

---

## ðŸŽ¯ Components Created

### 1. MultiTaskPredictor Class
**File:** `src/inference/multi_task_predictor.py` (525 lines)

Unified inference wrapper for both classification and segmentation.

**Key Methods:**
```python
# Conditional prediction (recommended)
result = predictor.predict_conditional(image)
# Returns segmentation only if tumor_prob >= 0.3

# Full prediction with Grad-CAM
result = predictor.predict_full(image, include_gradcam=True)

# Batch processing
results = predictor.predict_batch(images)
```

### 2. Production Configuration
**File:** `configs/multi_task_production.yaml` (275 lines)

Complete configuration including:
- Model architecture (base_filters=32, depth=3)
- Thresholds (classification=0.3, segmentation=0.5)
- Performance metrics from Phase 3
- Clinical recommendations

### 3. FastAPI Backend Integration
**File:** `app/backend/main_v2.py` (updated)

**New Endpoint:**
```
POST /predict_multitask
```

**Updated Endpoints:**
```
GET /healthz          # Now includes multitask_loaded
GET /model/info       # Now includes multitask performance
```

### 4. Streamlit UI Enhancement
**File:** `app/frontend/app_v2.py` (updated)

**New Tab:** "ðŸŽ¯ Multi-Task" (first tab)

Features:
- Conditional display logic
- Performance metrics
- Comprehensive visualizations
- Clinical interpretation

### 5. Demo Launcher
**File:** `scripts/run_multitask_demo.py` (344 lines)

One-command demo deployment with health checks and automatic browser opening.

---

## ðŸš€ Quick Start

### Prerequisites
```bash
# Ensure multi-task model is trained
ls checkpoints/multitask_joint/best_model.pth
```

### Launch Demo
```bash
python scripts/run_multitask_demo.py
```

The script will:
1. âœ… Check if checkpoint exists
2. âœ… Start backend server (port 8000)
3. âœ… Wait for backend health check
4. âœ… Start frontend server (port 8501)
5. âœ… Open browser automatically

### Manual Launch
```bash
# Terminal 1: Backend
python -m uvicorn app.backend.main_v2:app --host localhost --port 8000 --reload

# Terminal 2: Frontend
streamlit run app/frontend/app_v2.py --server.port 8501
```

---

## ðŸ“¡ API Endpoint Documentation

### POST /predict_multitask

Unified multi-task prediction with conditional segmentation.

**Request:**
```bash
curl -X POST "http://localhost:8000/predict_multitask?include_gradcam=true" \
  -F "file=@mri_slice.png"
```

**Response:**
```json
{
  "classification": {
    "predicted_class": 1,
    "predicted_label": "Tumor",
    "confidence": 0.92,
    "tumor_probability": 0.92,
    "probabilities": {
      "No Tumor": 0.08,
      "Tumor": 0.92
    }
  },
  "segmentation_computed": true,
  "segmentation": {
    "mask_available": true,
    "tumor_area_pixels": 1234,
    "tumor_percentage": 1.88,
    "mask_base64": "...",
    "prob_map_base64": "...",
    "overlay_base64": "..."
  },
  "recommendation": "Tumor detected with 92.0% confidence. Segmentation mask generated.",
  "gradcam_overlay": "base64_image...",
  "processing_time_ms": 45.23
}
```

**Conditional Logic:**
- If `tumor_probability < 0.3`: Segmentation not computed
- If `tumor_probability >= 0.3`: Full segmentation generated

### GET /healthz

Health check with multi-task model status.

**Response:**
```json
{
  "status": "healthy",
  "classifier_loaded": true,
  "segmentation_loaded": true,
  "calibration_loaded": true,
  "multitask_loaded": true,
  "device": "cuda",
  "timestamp": "2025-12-06T17:40:00"
}
```

### GET /model/info

Comprehensive model information including multi-task performance.

**Response:**
```json
{
  "multitask": {
    "architecture": "Multi-Task U-Net",
    "parameters": {
      "encoder": 1172640,
      "seg_decoder": 775073,
      "cls_head": 66306,
      "total": 2014019
    },
    "tasks": ["classification", "segmentation"],
    "classification_threshold": 0.3,
    "segmentation_threshold": 0.5,
    "performance": {
      "classification_accuracy": 0.9130,
      "classification_sensitivity": 0.9714,
      "classification_roc_auc": 0.9184,
      "segmentation_dice": 0.7650,
      "segmentation_iou": 0.6401,
      "combined_metric": 0.8390
    }
  }
}
```

---

## ðŸ’» Usage Examples

### Python API Client

```python
import requests
from PIL import Image
import io

# Load image
image = Image.open("mri_slice.png")
img_bytes = io.BytesIO()
image.save(img_bytes, format='PNG')
img_bytes.seek(0)

# Call API
files = {'file': ('image.png', img_bytes, 'image/png')}
params = {'include_gradcam': True}

response = requests.post(
    "http://localhost:8000/predict_multitask",
    files=files,
    params=params
)

result = response.json()

# Access results
tumor_prob = result['classification']['tumor_probability']
print(f"Tumor probability: {tumor_prob*100:.1f}%")

if result['segmentation_computed']:
    tumor_area = result['segmentation']['tumor_area_pixels']
    print(f"Tumor area: {tumor_area} pixels")
else:
    print("Segmentation not computed (low tumor probability)")
```

### Direct Inference

```python
from src.inference.multi_task_predictor import create_multi_task_predictor
import numpy as np
from PIL import Image

# Create predictor
predictor = create_multi_task_predictor(
    checkpoint_path='checkpoints/multitask_joint/best_model.pth',
    base_filters=32,
    depth=3,
    classification_threshold=0.3,
    segmentation_threshold=0.5
)

# Load image
image = np.array(Image.open("mri_slice.png"))

# Conditional prediction (recommended)
result = predictor.predict_conditional(image)

print(f"Tumor probability: {result['classification']['tumor_probability']}")
print(f"Segmentation computed: {result['segmentation_computed']}")
print(f"Recommendation: {result['recommendation']}")

# Full prediction with Grad-CAM
result = predictor.predict_full(image, include_gradcam=True)

if 'gradcam' in result:
    heatmap = result['gradcam']['heatmap']
    # Use heatmap for visualization
```

---

## ðŸŽ¨ UI Usage

### Multi-Task Tab

1. **Navigate** to the "ðŸŽ¯ Multi-Task" tab (first tab)
2. **Upload** an MRI slice (PNG, JPG, JPEG)
3. **Enable/Disable** Grad-CAM visualization (optional)
4. **Click** "Run Multi-Task Prediction"

### Results Display

**Classification Results:**
- Tumor probability with color coding
- Confidence score
- Probability distribution chart
- Recommendation

**Segmentation Results (Conditional):**
- Only shown if tumor_prob >= 30%
- Binary mask, probability map, overlay
- Tumor statistics (area, percentage)

**Grad-CAM Visualization:**
- Attention heatmap
- Side-by-side comparison

**Clinical Interpretation:**
- High confidence (>70%): Immediate review recommended
- Moderate (30-70%): Follow-up imaging
- Low (<30%): Routine monitoring

---

## ðŸ”§ Troubleshooting

### Issue: Multi-task model not loaded

**Symptom:**
```
âœ— Multi-task model not found at checkpoints/multitask_joint/best_model.pth
```

**Solution:**
```bash
# Train the multi-task model
python scripts/train_multitask_joint.py
```

### Issue: Port already in use

**Symptom:**
```
âœ— Port 8000 is already in use
```

**Solution:**
```bash
# Find and kill process on port 8000 (Windows)
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Or change port in run_multitask_demo.py
BACKEND_PORT = 8001
```

### Issue: Backend fails to start

**Symptom:**
```
âœ— Backend failed to start within timeout
```

**Solution:**
1. Check if all dependencies are installed: `pip install -r requirements.txt`
2. Verify checkpoint exists
3. Check CUDA availability: `python -c "import torch; print(torch.cuda.is_available())"`
4. Review backend logs for detailed error

### Issue: Segmentation not computed

**Symptom:**
```
â„¹ï¸ Segmentation not computed (tumor probability 15.0% is below 30% threshold)
```

**Explanation:**
This is **expected behavior**. The model uses conditional segmentation to save resources. Segmentation is only computed when tumor probability >= 30%.

**To force segmentation:**
Modify `classification_threshold` in `configs/multi_task_production.yaml` or use `predict_single()` with `do_seg=True`.

### Issue: Out of memory

**Symptom:**
```
RuntimeError: CUDA out of memory
```

**Solution:**
1. Reduce batch size in config
2. Use CPU inference: `device='cpu'`
3. Close other GPU applications

---

## ðŸ“Š Performance Metrics

### Inference Speed

| Operation | Time (ms) | Speedup |
|-----------|-----------|---------|
| Separate models (cls + seg) | ~130 | - |
| Multi-task model | ~80 | 38% faster |
| Multi-task (conditional) | ~50 | 62% faster |

### Model Size

| Model | Parameters | Size (MB) |
|-------|-----------|-----------|
| Separate models | 2.2M | ~8.8 |
| Multi-task model | 2.0M | ~8.0 |
| **Reduction** | **9.4%** | **9.4%** |

### Accuracy

| Metric | Value |
|--------|-------|
| Classification Accuracy | 91.30% |
| Classification Sensitivity | 97.14% |
| Classification ROC-AUC | 0.9184 |
| Segmentation Dice | 76.50% Â± 13.97% |
| Segmentation IoU | 64.01% Â± 18.37% |
| **Combined Metric** | **83.90%** |

---

## ðŸ”„ Comparison with Separate Models

### Advantages
âœ… Faster inference (~40% speedup)  
âœ… Fewer parameters (9.4% reduction)  
âœ… Shared feature learning  
âœ… Conditional segmentation  
âœ… Single model deployment  

### Trade-offs
âš ï¸ Segmentation Dice slightly lower (76.5% vs 86.4%)  
âš ï¸ More complex training pipeline  
âš ï¸ Requires both datasets  

### When to Use Multi-Task
- Real-time inference required
- Resource-constrained deployment
- Need both classification and segmentation
- Screening applications (high sensitivity priority)

### When to Use Separate Models
- Maximum segmentation accuracy required
- Only one task needed
- Different update schedules for models

---

## ðŸ“ Configuration

### Key Parameters

**Model Architecture:**
```yaml
model:
  base_filters: 32      # Matches trained model
  depth: 3              # Matches trained model
  in_channels: 1        # Single channel (FLAIR)
```

**Inference Thresholds:**
```yaml
inference:
  classification_threshold: 0.3   # Show segmentation if >= 30%
  segmentation_threshold: 0.5     # Binary mask threshold
```

**API Settings:**
```yaml
api:
  host: "0.0.0.0"
  port: 8000
```

**UI Settings:**
```yaml
ui:
  port: 8501
  conditional:
    low_prob_threshold: 0.3
    high_prob_threshold: 0.7
```

---

## ðŸŽ“ Next Steps

1. **Test the demo:**
   ```bash
   python scripts/run_multitask_demo.py
   ```

2. **Try different images:**
   - Upload various MRI slices
   - Test conditional segmentation
   - Compare with separate model tabs

3. **Explore API:**
   - Visit `http://localhost:8000/docs`
   - Test endpoints with Swagger UI
   - Review response formats

4. **Customize thresholds:**
   - Modify `configs/multi_task_production.yaml`
   - Adjust classification_threshold
   - Test different confidence levels

5. **Integration:**
   - Use API in your application
   - Implement custom UI
   - Deploy to production

---

## ðŸ“š Related Documentation

- `IMPLEMENT_UNIFIED_ENCODER.md` - Full implementation plan
- `documentation/MULTITASK_EVALUATION_REPORT.md` - Phase 3 evaluation
- `documentation/PHASE2.3_QUICK_START.md` - Training guide
- `configs/multi_task_production.yaml` - Complete configuration

---

## ðŸ†˜ Support

**Issues?**
- Check troubleshooting section above
- Review API logs: Backend console output
- Test with sample images from `data/dataset_examples/`

**Questions?**
- Review evaluation report for performance details
- Check configuration file for all parameters
- Test individual components (predictor, API, UI)

---

**Phase 4 Status:** âœ… COMPLETE  
**Total Code:** ~1,554 lines  
**Ready for:** Production deployment and testing
